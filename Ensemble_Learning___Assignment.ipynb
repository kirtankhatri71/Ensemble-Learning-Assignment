{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "mJELGPoGEW6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "- Ensembling combines multiple models predictions which is more stable and accurate as copared to individual model.\n",
        "\n",
        " The key idea behind it is that single decision tree leads to overfitting and to control that we apply some hyperparameters but every time that doesn't works good. so we apply ensembling in which number of models make predictions and combining all the predictions, ensembling makes one prediction and improves accuracy."
      ],
      "metadata": {
        "id": "qYrehsUcElzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        " - Bagging makes all the models paralel and independent of each other.\n",
        " - Boosting makes sequetial models in which every model learns from the previous model"
      ],
      "metadata": {
        "id": "GJdggTDwFqu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        " - Bootsrap sampling means making multiple models but every time the data will be randomly selected.\n",
        "\n",
        "   Each model (tree) is trained on a different bootstrap sample, so the trees become less correlated. This helps reduce overfitting and improves overall accuracy."
      ],
      "metadata": {
        "id": "9jW3N3d4GF9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        " - out-of-Bag (OOB) samples are the data points not selected during bootstrap sampling.\n",
        "\n",
        "   How OOB score is used:\n",
        "   Each model (tree) predicts only its unused (OOB) samples. These predictions are combined to calculate the OOB score, which acts like a validation accuracy."
      ],
      "metadata": {
        "id": "6WhhjIVrGtUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        " - Decision Tree:\n",
        "Importance is based on splits in one tree, so it can be unstable and sensitive to data changes.\n",
        "\n",
        "- Random Forest:\n",
        "Importance is averaged over many trees, so it is more stable, reliable, and less biased."
      ],
      "metadata": {
        "id": "AYfalUUbHAOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ircYnmUUES3q",
        "outputId": "bc313fff-71a9-42ae-ded7-2f7d20236bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "                Feature  Importance\n",
            "23           worst area    0.159700\n",
            "20         worst radius    0.109397\n",
            "22      worst perimeter    0.087272\n",
            "2        mean perimeter    0.083987\n",
            "7   mean concave points    0.080582\n"
          ]
        }
      ],
      "source": [
        "# 6: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "x,y = data.data, data.target\n",
        "\n",
        "# train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=32)\n",
        "\n",
        "# model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "# Feature Importance\n",
        "\n",
        "important_df = pd.DataFrame({\n",
        "    \"Features\": data.feature_names,\n",
        "    \"Importance\": model.feature_importances_\n",
        "})\n",
        "\n",
        "# Top 5 important features\n",
        "top_5 = importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 most important features:\")\n",
        "print(top_5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load_dataset\n",
        "\n",
        "data = load_iris()\n",
        "x,y = data.data , data.target\n",
        "\n",
        "# train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=32)\n",
        "\n",
        "# model with baging and decision tree\n",
        "base_tree = DecisionTreeClassifier()\n",
        "bagging_model = BaggingClassifier(estimator = base_tree, n_estimators = 15)\n",
        "bagging_model.fit(x_train,y_train)\n",
        "print(\"Accuracy:\",accuracy_score(y_test,bagging_model.predict(x_test)))\n",
        "\n",
        "# model with sngle decision tree\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(x_train,y_train)\n",
        "print(\"Accuracy:\",accuracy_score(y_test,model.predict(x_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6T_uHMIIYLK",
        "outputId": "249e7751-7174-4b9b-ef1e-1b37ad9b1315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n",
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8: Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load_dataset\n",
        "\n",
        "data = load_iris()\n",
        "x,y = data.data , data.target\n",
        "\n",
        "# train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=32)\n",
        "\n",
        "# model\n",
        "hyperparam = ({\n",
        "    \"max_depth\":[2,3,4,5,6,7,10],\n",
        "    \"n_estimators\":[5,10,20,30,50,70,100]\n",
        "})\n",
        "\n",
        "grid_search = GridSearchCV(estimator =RandomForestClassifier(random_state=32), param_grid = hyperparam, cv = 5, scoring=\"accuracy\",n_jobs = -1)\n",
        "grid_search.fit(x_train,y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_search.best_estimator_.predict(x_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrw3RwUWJuu6",
        "outputId": "a880cf83-c970-48fa-e7f5-c5a120c30c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 6, 'n_estimators': 5}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# load data\n",
        "data = fetch_california_housing()\n",
        "x,y = data.data , data.target\n",
        "\n",
        "#train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=32)\n",
        "\n",
        "\n",
        "#model\n",
        "bag_model = BaggingRegressor(n_estimators = 100, random_state = 32)\n",
        "bag_model.fit(x_train,y_train)\n",
        "print(\"MSE:\",mean_squared_error(y_test,bag_model.predict(x_test)))\n",
        "\n",
        "# model 2\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators = 100, random_state = 34)\n",
        "rf_model.fit(x_train,y_train)\n",
        "print(\"MSE:\",mean_squared_error(y_test,rf_model.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOs3nwlELPcV",
        "outputId": "7918c2a8-2bce-42f7-c8ba-06a5f9806a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.25357605123436444\n",
            "MSE: 0.2575844635105253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "- I choose bagging over boosting because all the transaction history data processed paralelly their is no need for sequential learning from previvous data.\n",
        "\n",
        "- To control overfitting tune some hyperparameters like max_depth, min_samples_split and min_samples_leaf.\n",
        "\n",
        "- Decision tree is the best base model because their is noise and non-linear data. And decision tree gracefully handles the data.\n",
        "\n",
        "- Split the data into K parts\n",
        "\n",
        "  Train on K-1 parts, test on 1 part (repeat for all parts)\n",
        "\n",
        "   Check ROC-AUC, Precision, and Recall\n",
        "\n",
        "  Compare single Decision Tree vs Bagged model\n",
        "\n",
        "- By training(making) many models and take predictions of all the models as combine in one. This helps alot because we are not depend only on one model, the model is more stable and accurate.\n",
        "In finance, small accuracy leads to large financial impact, making ensemble highly value."
      ],
      "metadata": {
        "id": "mn57hJueOQN2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sBCK0owNChX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}